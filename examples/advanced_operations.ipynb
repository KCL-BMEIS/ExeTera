{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a133c850-1cc3-4ee5-830a-9440e70cd90c",
   "metadata": {},
   "source": [
    "This example uses the user_assessments hdfs file from RandomDataset. User assessments file contains a user table and a assessments table, that imitate the data structure of in CSS (Covid Symptom Study) project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06eae4-8214-4e96-a419-d361698825a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae8d06-0872-4e30-b6c8-2e8ba86fe9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exetera.core.session import Session\n",
    "s = Session()  # not recommended, but to cover all the cells in the example, we use this way here\n",
    "src = s.open_dataset('user_assessments.hdf5', 'r', 'src')\n",
    "print(src.keys())\n",
    "users = src['users']\n",
    "print('Columns in users table:', users.keys())\n",
    "# use describe to check the value in each column\n",
    "users.describe(include=['bmi', 'has_diabetes', 'height_cm',  'year_of_birth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d79440-4c2d-42ec-8f62-3d10bc72e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "asmts = src['assessments']\n",
    "print('Columns in users table:', asmts.keys())\n",
    "asmts.describe(include=['abdominal_pain', 'brain_fog', 'date','loss_of_smell', 'temperature_f'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be7ec97-a8d2-449f-9f57-e54a4effb52c",
   "metadata": {},
   "source": [
    "<h3>Filtering</h3>\n",
    "Filtering is performed through the use of the apply_filter function. This can be performed on <b>individual fields</b> or at a <b>dataframe level</b>. apply_filter applies the filter on data rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8e873-cf1f-47c5-bebb-18bde4357543",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session() as s:\n",
    "    dst = s.open_dataset('temp2.hdf5', 'w', 'dst')\n",
    "    df = dst.create_dataframe('df')\n",
    "\n",
    "    # apply a filter to the dataframe\n",
    "\n",
    "    filt = (2022 - users['year_of_birth'].data[:]) > 18\n",
    "    users.apply_filter(filt, ddf=df)  # non-destructive with ddf argument\n",
    "    print(len(df['id']), ' adults out of ', len(users['id']), ' total subjects found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c336d758-878a-4df6-8458-cc3ddf280964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining filters\n",
    "# we can make use of fields directly rather than fetching the underlying numpy arrays\n",
    "# we recommend this approach in general\n",
    "\n",
    "filt = ((2022 - users['year_of_birth'].data[:]) > 18) & (users['has_diabetes'].data[:] == False)\n",
    "print(filt)\n",
    "\n",
    "# fetching numpy arrays\n",
    "print(users['id'].data[filt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316eb91-2b59-46d9-9f4f-1262192d6807",
   "metadata": {},
   "source": [
    "<h3>Performance boost using numba</h3>\n",
    "As the underlying data is fetched as a numpy array, you can utlize the numba @njit functions to accelarate the data process. For example in the case of summing up symptoms, use a seperate function with @njit decrator can speed up the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f52b9-f96d-4e36-a8e3-67953aaedae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#sum up the symptoms without njit\n",
    "test_length = 1000000000  # here we use the a test length rather than 50 rows in the dataset, \n",
    "                            # as the difference comes with more rows\n",
    "symptoms = ['abdominal_pain', 'brain_fog',  'loss_of_smell']\n",
    "t0 = time.time()\n",
    "sum_symp = np.zeros(test_length, 'int32')\n",
    "for i in symptoms:\n",
    "    sum_symp += np.zeros(test_length, 'int32')\n",
    "#print(sum_symp)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202321d4-b1fc-47bd-8878-779df121c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum up the symptoms with njit\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def sum_symptom(symp_data, sum_data):\n",
    "    sum_data += symp_data\n",
    "    return sum_data\n",
    "\n",
    "t0 = time.time()\n",
    "sum_symp = np.zeros(test_length, 'int32')\n",
    "for i in symptoms:\n",
    "    sum_symp = np.zeros(test_length, 'int32')\n",
    "#print(sum_symp)\n",
    "print(time.time()-t0)  # 10x faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723a9c9-36a5-4737-870c-7b4d130307f8",
   "metadata": {},
   "source": [
    "<h3>Groupby</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfec7b4-01ec-4bf5-b8d3-d99a9db98273",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session() as s:\n",
    "    dst = s.open_dataset('temp2.hdf5', 'w', 'dst')\n",
    "    df = dst.create_dataframe('df')\n",
    "    #drop duplicates\n",
    "    asmts.drop_duplicates(by = 'user_id', ddf = df)\n",
    "    print(len(df['user_id']), len(asmts['user_id']))\n",
    "    \n",
    "    #count\n",
    "    df2 = dst.create_dataframe('df2')\n",
    "    asmts.groupby(by = 'user_id').count(ddf = df2)\n",
    "    print(len(df2['user_id']), len(asmts['user_id']))\n",
    "    \n",
    "    #min/ max\n",
    "    df3 = dst.create_dataframe('df3')\n",
    "    asmts.groupby(by = 'user_id').max(target ='date', ddf = df3)\n",
    "    print(len(df3['user_id']), len(asmts['user_id']))\n",
    "    df4 = dst.create_dataframe('df4')\n",
    "    asmts.groupby(by = 'user_id').min(target ='date', ddf = df4)\n",
    "    print(len(df4['user_id']), len(asmts['user_id']))\n",
    "\n",
    "    #first/last\n",
    "    df5 = dst.create_dataframe('df5')\n",
    "    asmts.groupby(by = 'user_id').first(target ='date', ddf = df5)\n",
    "    df6 = dst.create_dataframe('df6')\n",
    "    asmts.groupby(by = 'user_id').last(target ='date', ddf = df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef8595-6835-4f0d-a5bf-6ee15bf8eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform rather than group by\n",
    "with Session() as s:\n",
    "    dst = s.open_dataset('temp2.hdf5', 'w', 'dst')\n",
    "    df = dst.create_dataframe('df')\n",
    "    \n",
    "    symptoms = ['abdominal_pain', 'brain_fog',  'loss_of_smell']\n",
    "    sum_symp = np.zeros(len(asmts['user_id']), 'int32')\n",
    "    for i in symptoms:\n",
    "        sum_symp += np.zeros(len(asmts['user_id']), 'int32')\n",
    "    \n",
    "    spans = asmts['user_id'].get_spans()  # make sure asmts dataframe is sorted based on user_id\n",
    "    max_symp = np.zeros(len(asmts['user_id']), 'int32')\n",
    "    for i in range(len(spans)-1):\n",
    "        max_symp[spans[i]:spans[i+1]] = np.max(sum_symp.data[spans[i]:spans[i+1]])\n",
    "    #write data back to df\n",
    "    df.create_numeric('max_symp', 'int32').data.write(max_symp)\n",
    "    print(len(df['max_symp'].data))  # note the field length is the same with transform\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e5314b-d130-4ded-a41a-ca7fda798ae1",
   "metadata": {},
   "source": [
    "<h3>Join</h3>\n",
    "ExeTera provides functions that provide pandas-like merge functionality on DataFrame instances. We have made this operation as familiar as possible to Pandas users, but there are a couple of differences that we should highlight:\n",
    "<br>\n",
    "\n",
    "&bull; merge is provided as a function in the dataframe unit, rather than as a member function on DataFrame instances \n",
    "<br>\n",
    "&bull; merge takes three dataframe arguments, left, right and dest. This is due to the fact that DataFrames are always backed up by a datastore and so rather than create an in-memory destination dataframe, the resulting merged fields must be written to a dataframe of your choosing. \n",
    "<br>\n",
    "&bull; Note, this can either be a separate dataframe or it can be the dataframe that you are merging to (typically left in the case of a \"left\" merge and right in the case of a \"right\" merge\n",
    "<br>\n",
    "&bull; merge takes a number of optional hint fields that can save time when working with large datasets. These specify whether the keys are unique or ordered and allow the merge to occur without first checking this\n",
    "<br>\n",
    "&bull; merge has a number of highly scalable algorithms that can be used when the key data is sorted and / or unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85261f6a-ba35-45a0-ba88-60a99a16ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exetera.core.dataframe import merge\n",
    "with Session() as s:\n",
    "    dst = s.open_dataset('temp2.hdf5', 'w', 'dst')\n",
    "    df = dst.create_dataframe('df')\n",
    "    merge(users, asmts, df, left_on='id', right_on='user_id', how='left')\n",
    "    print(len(df['id_l'].data))  # note as there are 'id' field in both dataframe, thus a suffix '_l' and '_r'\n",
    "                                    # are added to the merged dataframe \n",
    "    print(df.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c862b-9a2a-4e08-8a5b-d7c2cadaf7b1",
   "metadata": {},
   "source": [
    "<h3>Sort</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72666d8-a5e3-48e9-859d-1d1e56b4a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exetera.core.dataframe import merge\n",
    "with Session() as s:\n",
    "    dst = s.open_dataset('temp2.hdf5', 'w', 'dst')\n",
    "    df = dst.create_dataframe('df')\n",
    "    merge(users, asmts, df, left_on='id', right_on='user_id', how='left')\n",
    "    s.sort_on(df, df, ('id_l',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77e233-16ed-4333-bec8-e8bc8f0e297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exetera.core.dataframe import merge\n",
    "with Session() as s:\n",
    "    dst = s.open_dataset('temp2.hdf5', 'w', 'dst')\n",
    "    df = dst.create_dataframe('df')\n",
    "    merge(users, asmts, df, left_on='id', right_on='user_id', how='left')\n",
    "    df2 = dst.create_dataframe('df2')\n",
    "    s.sort_on(df, df2, ('id_l',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1396c8-e851-4e77-ba43-9ec4f9d01062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting with an index\n",
    "with Session() as s:\n",
    "    dst = s.open_dataset('temp2.hdf5', 'w', 'dst')\n",
    "    df = dst.create_dataframe('df')\n",
    "    merge(users, asmts, df, left_on='id', right_on='user_id', how='left')\n",
    "\n",
    "    index = s.dataset_sort_index((df['id_l'],))\n",
    "\n",
    "    # apply indices to a destination dataframe\n",
    "    df2 = dst.create_dataframe('df2')\n",
    "    df.apply_index(index, df2)\n",
    "    print(df2['id_l'].data[:])\n",
    "    \n",
    "    # apply indices in place\n",
    "    df.apply_index(index)\n",
    "    print(df['id_l'].data[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2a9e4-400a-44e9-b08e-ef5f282bf6bd",
   "metadata": {},
   "source": [
    "<h3>I/O</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c67e89-1651-41b6-9f76-697875a82d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session() as s:\n",
    "    dst = s.open_dataset('temp2.hdf5', 'w', 'dst')\n",
    "    df = dst.create_dataframe('df')\n",
    "    merge(users, asmts, df, left_on='id', right_on='user_id', how='left')\n",
    "\n",
    "    #output a dataframe to to_csv\n",
    "    df.to_csv('merged.csv')\n",
    "\n",
    "    #output to csv with row filters\n",
    "    row_filter = (2022-df['year_of_birth'].data[:]) > 18\n",
    "    df.to_csv('adults.csv', row_filter)  # save the data you want without change the underlying data in df\n",
    "\n",
    "    #output to csv with column filters\n",
    "    df.to_csv('column_filtered.csv', column_filter=['id_l', 'year_of_birth', 'date', 'tested_covid_positive'])  # save the columns you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c08d25-f2ee-41c1-a5e8-73a34f5d94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d47e2-7237-4cd4-938a-4ad50d2ceda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close src dataset as we open dataset using s=Session()\n",
    "# this is not necessary if we use context management by with Session as s:\n",
    "s.close_dataset(src)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
